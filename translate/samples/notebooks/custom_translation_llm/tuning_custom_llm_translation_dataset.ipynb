{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oHrrDiHwtm8y"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2024 Google LLC.\n",
        "\n",
        "SPDX-License-Identifier: Apache-2.0"
      ],
      "metadata": {
        "id": "xB1tWxGiHlnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Colab example initiates a tuning job on base model `translation-llm-002` with provided [translation dataset](https://cloud.google.com/translate/docs/advanced/automl-quickstart?_gl=1*1uxuas*_up*MQ..&gclid=Cj0KCQiAvP-6BhDyARIsAJ3uv7bZUmDC7sTZ4d5N-08d2vtOXi9smnSTsI0YDQHK49fxYKwy68eAAtMaAixREALw_wcB&gclsrc=aw.ds) (legacy datasets are not supported)."
      ],
      "metadata": {
        "id": "5EC94Xly2pmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prerequisites"
      ],
      "metadata": {
        "id": "H6RvoTU-zSvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Enable Vertex AI API in APIs & services page.\n",
        "*   It's recommended to use Colab for running the script, as it best supports the authentication process and Cloud CLI."
      ],
      "metadata": {
        "id": "L-tS_wwgzSvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prerequisite: Enable Vertex AI API in APIs & services page."
      ],
      "metadata": {
        "id": "1tD2h_utjnrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Authentication"
      ],
      "metadata": {
        "id": "pMpx_VgHokrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "\n",
        "PROJECT_ID = \"my-project\"  # @param {type:\"string\"}\n",
        "auth.authenticate_user(project_id=PROJECT_ID)"
      ],
      "metadata": {
        "id": "NCgN8AQhYRdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project {PROJECT_ID}"
      ],
      "metadata": {
        "id": "P-JukDsyTyad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Input parameters"
      ],
      "metadata": {
        "id": "Q6xZ3eVVpDhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quick start: First import a native dataset at [AutoML](https://console.cloud.google.com/translation/datasets), and get dataset id under the display name. Fill in the required parameters.\n",
        "\n",
        "By default, the model name to be used for translate text requests will be returned after the tuning finishes. For your reference, the tuning will take less than 20 minutes for a dataset with 10k training examples."
      ],
      "metadata": {
        "id": "W5d3FdGv2Pli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to save converted dataset.\n",
        "GCS_EXPORT_PATH = 'gs://my_bucket/dir' # @param {type:\"string\"}\n",
        "\n",
        "# Only native datasets are supported.\n",
        "DATASET_ID = '123abc' # @param {type:\"string\"}\n",
        "\n",
        "# Model display name on Vertex AI Online Prediction page.\n",
        "TUNED_MODEL_DISPLAY_NAME = 'translation-llm-test' # @param {type:\"string\"}\n",
        "\n",
        "# Set sample size. Set to \"-1\" to use all examples.\n",
        "TRAIN_DATASET_SAMPLE_SIZE = -1 # @param {type:\"integer\"}\n",
        "\n",
        "# Validation size limit is 1000.\n",
        "VALIDATION_DATASET_SAMPLE_SIZE = 250 # @param {type:\"integer\"}\n"
      ],
      "metadata": {
        "id": "cs_GIMWVnAoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Helper functions"
      ],
      "metadata": {
        "id": "HkWlLSwxpgBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only us-central1 is supported for now\n",
        "LOCATION = 'us-central1'\n",
        "\n",
        "language_map = {\n",
        "    'en' : 'English',\n",
        "    'es' : 'Spanish',\n",
        "    'fr' : 'French',\n",
        "    'de' : 'German',\n",
        "    'it' : 'Italian',\n",
        "    'pt' : 'Portuguese',\n",
        "    'zh' : 'Chinese',\n",
        "    'ja' : 'Japanese',\n",
        "    'ko' : 'Korean',\n",
        "    'ar' : 'Arabic',\n",
        "    'hi' : 'Hindi',\n",
        "    'ru' : 'Russian',\n",
        "}"
      ],
      "metadata": {
        "id": "3EqyZ4ZDfpTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "from google.cloud import translate_v3\n",
        "from google.cloud import storage\n",
        "\n",
        "import vertexai\n",
        "from vertexai.tuning import sft\n",
        "\n",
        "\n",
        "# Creates single json tuning input data\n",
        "def convert_line_to_jsonl(source_language, target_language, source_sentence, target_sentence):\n",
        "  return json.dumps({\n",
        "      \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": source_language + \": \" + source_sentence + \" \" + target_language + \": \"}]},\n",
        "       {\"role\": \"model\", \"parts\": [{\"text\": target_sentence}]}]}, ensure_ascii=False)\n",
        "\n",
        "\n",
        "# Checking dataset exists and extract language pairs\n",
        "def check_dataset(project_id, location, dataset_id):\n",
        "  translation_client = translate_v3.TranslationServiceClient()\n",
        "  request = translate_v3.GetDatasetRequest(\n",
        "    name=f\"projects/{project_id}/locations/{location}/datasets/{dataset_id}\",\n",
        "  )\n",
        "  try:\n",
        "    response = translation_client.get_dataset(request=request)\n",
        "    print(response)\n",
        "    if response.source_language_code not in language_map or response.target_language_code not in language_map:\n",
        "      raise ValueError(\"Invalid language code\")\n",
        "    return response.source_language_code, response.target_language_code\n",
        "  except Exception as e:\n",
        "    raise ValueError(f\"Error getting dataset: {e}\")\n",
        "\n",
        "\n",
        "# Export dataset to gcs directory\n",
        "def export_data(project_id, location, dataset_id, gcs_export_path):\n",
        "  translation_client = translate_v3.TranslationServiceClient()\n",
        "\n",
        "  # Initialize request argument(s)\n",
        "  output_config = translate_v3.DatasetOutputConfig()\n",
        "  output_config.gcs_destination.output_uri_prefix = gcs_export_path\n",
        "\n",
        "  export_request = translate_v3.ExportDataRequest(\n",
        "    dataset=f\"projects/{project_id}/locations/{location}/datasets/{dataset_id}\",\n",
        "    output_config=output_config,\n",
        "  )\n",
        "\n",
        "  # Make the request\n",
        "  response = translation_client.export_data(request=export_request)\n",
        "\n",
        "  print(\"Waiting for operation to complete...\")\n",
        "\n",
        "  while not response.done():\n",
        "    time.sleep(5)\n",
        "\n",
        "  if response.metadata.error.message:\n",
        "    print(\"Dataset exported failed.\")\n",
        "    print(response.metadata.error.message)\n",
        "    return \"\"\n",
        "  else:\n",
        "    print(\"Dataset exported successfully.\")\n",
        "    operation_short_name = response.operation.name.rsplit('/', 1)[-1]\n",
        "    exported_bucket = gcs_export_path + '/exported_' + dataset_id + '_' + operation_short_name\n",
        "    print(exported_bucket)\n",
        "    return exported_bucket\n",
        "\n",
        "\n",
        "# Format conversion function as part of the AutoML export workflow.\n",
        "def convert_exported_files(colab_path, source_language_code, target_language_code, train_dataset_sample_size, validation_dataset_sample_size):\n",
        "  train_file_list = glob.glob(colab_path + '/train*')\n",
        "  validation_file_list = glob.glob(colab_path + '/validation*')\n",
        "  train_jsonl = os.path.join(colab_path, \"train.jsonl\")\n",
        "  validation_jsonl = os.path.join(colab_path, \"validation.jsonl\")\n",
        "\n",
        "  with open(train_jsonl, 'w', encoding='utf-8') as outfile:\n",
        "    count = 0\n",
        "    for train_file in train_file_list:\n",
        "      with open(train_file, 'r', encoding='utf-8') as infile:\n",
        "        reader = csv.reader(infile, delimiter='\\t')\n",
        "        for row in reader:\n",
        "          message = convert_line_to_jsonl(language_map[source_language_code], language_map[target_language_code], row[0], row[1])\n",
        "          outfile.write(message)\n",
        "          outfile.write('\\n')\n",
        "          count += 1\n",
        "          if count == train_dataset_sample_size:\n",
        "            break\n",
        "      if count == train_dataset_sample_size:\n",
        "        break\n",
        "\n",
        "  with open(validation_jsonl, 'w', encoding='utf-8') as outfile:\n",
        "    count = 0\n",
        "    for validation_file in validation_file_list:\n",
        "      with open(validation_file, 'r', encoding='utf-8') as infile:\n",
        "        reader = csv.reader(infile, delimiter='\\t')\n",
        "        for row in reader:\n",
        "          message = convert_line_to_jsonl(language_map[source_language_code], language_map[target_language_code], row[0], row[1])\n",
        "          outfile.write(message)\n",
        "          outfile.write('\\n')\n",
        "          count += 1\n",
        "          if count == validation_dataset_sample_size:\n",
        "            break\n",
        "      if count == validation_dataset_sample_size:\n",
        "        break\n",
        "\n",
        "  print(\"File conversion completed.\")\n",
        "\n",
        "\n",
        "# Initiates model training\n",
        "def train_model(train_dataset_path, validation_dataset_path, tuned):\n",
        "  vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "  sft_tuning_job = sft.train(\n",
        "    source_model=\"translation-llm-002\",\n",
        "    train_dataset=train_dataset_path,\n",
        "    validation_dataset=validation_dataset_path,\n",
        "    tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\n",
        "  )\n",
        "\n",
        "  # Polling for job completion\n",
        "  while not sft_tuning_job.has_ended:\n",
        "    time.sleep(60)\n",
        "    sft_tuning_job.refresh()\n",
        "\n",
        "  endpoint_short_name = sft_tuning_job.tuned_model_endpoint_name.rsplit('/', 1)[-1]\n",
        "  custom_model_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/models/translation-llm-custom/{endpoint_short_name}\"\n",
        "\n",
        "  print(\"Model: \", custom_model_name)\n",
        "  return custom_model_name\n"
      ],
      "metadata": {
        "id": "dTMTU9SYd3dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export dataset from AutoML"
      ],
      "metadata": {
        "id": "oHrrDiHwtm8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GCS_EXPORT_PATH = GCS_EXPORT_PATH.rstrip('/')\n",
        "SOURCE_LANGUAGE_CODE, TARGET_LANGUAGE_CODE = check_dataset(PROJECT_ID, LOCATION, DATASET_ID)\n",
        "exported_bucket = export_data(PROJECT_ID, LOCATION, DATASET_ID, GCS_EXPORT_PATH)"
      ],
      "metadata": {
        "id": "WjW6GkxvyEh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp -r {exported_bucket} '/content/'"
      ],
      "metadata": {
        "id": "PJe8kxrK5p7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Optional: Delete exported files in gcs bucket."
      ],
      "metadata": {
        "id": "KvfYGuTWj318"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil rm -r {exported_bucket}"
      ],
      "metadata": {
        "id": "_zKufnn0jXPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset Format Conversion"
      ],
      "metadata": {
        "id": "2mzgIOtOqJAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step convers data to `.jsonl` format for tuning."
      ],
      "metadata": {
        "id": "t-fj4DU6Bsv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colab_path = os.path.join('/content/', exported_bucket.rsplit('/', 1)[-1])\n",
        "convert_exported_files(colab_path, SOURCE_LANGUAGE_CODE, TARGET_LANGUAGE_CODE, TRAIN_DATASET_SAMPLE_SIZE, VALIDATION_DATASET_SAMPLE_SIZE)\n",
        "train_jsonl = os.path.join(colab_path, 'train.jsonl')\n",
        "validation_jsonl = os.path.join(colab_path, 'validation.jsonl')\n",
        "train_dataset_path = GCS_EXPORT_PATH + '/' + DATASET_ID + '_train.jsonl'\n",
        "validation_dataset_path = GCS_EXPORT_PATH + '/' + DATASET_ID + '_validation.jsonl'"
      ],
      "metadata": {
        "id": "5FBPjzlDB376"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp {train_jsonl} {train_dataset_path}\n",
        "!gsutil cp {validation_jsonl} {validation_dataset_path}"
      ],
      "metadata": {
        "id": "OUy_iz3yCSGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Optional: Remove dataset copied to Colab"
      ],
      "metadata": {
        "id": "tugoBkAv273E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf {colab_path}"
      ],
      "metadata": {
        "id": "jQns2hUb3CI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiate Vertex Tuning Request"
      ],
      "metadata": {
        "id": "-guLLW3wV8Ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tuning is done, the translation model name will be returned to be used for translation requests."
      ],
      "metadata": {
        "id": "wZVCR7wiB6Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model_name = train_model(train_dataset_path, validation_dataset_path, TUNED_MODEL_DISPLAY_NAME)"
      ],
      "metadata": {
        "id": "ARvHS1BLWIHX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}