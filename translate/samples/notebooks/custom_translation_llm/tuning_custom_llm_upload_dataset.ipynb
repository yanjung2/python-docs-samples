{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2024 Google LLC.\n",
        "\n",
        "SPDX-License-Identifier: Apache-2.0"
      ],
      "metadata": {
        "id": "xB1tWxGiHlnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Colab example initiates a tuning job on base model `translation-llm-002` with provided tsv, csv or tmx datasets."
      ],
      "metadata": {
        "id": "5EC94Xly2pmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prerequisites"
      ],
      "metadata": {
        "id": "kX7HGPMnnmq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Enable Vertex AI API in APIs & services page.\n",
        "*   It's recommended to use Colab for running the script, as it best supports the authentication process and Cloud CLI."
      ],
      "metadata": {
        "id": "1tD2h_utjnrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Authentication"
      ],
      "metadata": {
        "id": "pMpx_VgHokrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "\n",
        "PROJECT_ID = \"my-project\"  # @param {type:\"string\"}\n",
        "auth.authenticate_user(project_id=PROJECT_ID)"
      ],
      "metadata": {
        "id": "NCgN8AQhYRdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project {PROJECT_ID}"
      ],
      "metadata": {
        "id": "P-JukDsyTyad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Input parameters"
      ],
      "metadata": {
        "id": "Q6xZ3eVVpDhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quick start: Upload your input datasets to Colab. Fill in required parameters.\n",
        "\n",
        "By default, the model name to be used for translate text requests will be returned after the tuning finishes. For your reference, the tuning will take less than 20 minutes for a dataset with 10k training examples."
      ],
      "metadata": {
        "id": "W5d3FdGv2Pli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to save converted dataset.\n",
        "GCS_UPLOAD_PATH = 'gs://my_bucket/dir' # @param {type:\"string\"}\n",
        "\n",
        "# Model display name on Vertex AI Online Prediction page.\n",
        "TUNED_MODEL_DISPLAY_NAME = 'translation-llm-test' # @param {type:\"string\"}\n",
        "\n",
        "SOURCE_LANGUAGE_CODE = 'en' # @param {type:\"string\"}\n",
        "TARGET_LANGUAGE_CODE = 'es' # @param {type:\"string\"}\n",
        "\n",
        "# Colab path for train/validation data.\n",
        "# tsv, csv and tmx are supported.\n",
        "TRAIN_FILE_PATH = '/content/train.tsv' # @param {type:\"string\"}\n",
        "VALIDATION_FILE_PATH = '/content/validation.tsv' # @param {type:\"string\"}\n",
        "\n",
        "# Set sample size. Set to \"-1\" to use all examples.\n",
        "TRAIN_DATASET_SAMPLE_SIZE = -1 # @param {type:\"integer\"}\n",
        "\n",
        "# Validation size limit is 1000.\n",
        "VALIDATION_DATASET_SAMPLE_SIZE = 250 # @param {type:\"integer\"}\n"
      ],
      "metadata": {
        "id": "cs_GIMWVnAoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Helper functions"
      ],
      "metadata": {
        "id": "HkWlLSwxpgBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade translate-toolkit"
      ],
      "metadata": {
        "id": "HBndCkQheott"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only us-central1 is supported for now\n",
        "LOCATION = 'us-central1'\n",
        "\n",
        "language_map = {\n",
        "    'en' : 'English',\n",
        "    'es' : 'Spanish',\n",
        "    'fr' : 'French',\n",
        "    'de' : 'German',\n",
        "    'it' : 'Italian',\n",
        "    'pt' : 'Portuguese',\n",
        "    'zh' : 'Chinese',\n",
        "    'ja' : 'Japanese',\n",
        "    'ko' : 'Korean',\n",
        "    'ar' : 'Arabic',\n",
        "    'hi' : 'Hindi',\n",
        "    'ru' : 'Russian',\n",
        "}"
      ],
      "metadata": {
        "id": "ipfVLnX4_FQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from translate.storage.tmx import tmxfile\n",
        "\n",
        "from google.cloud import translate_v3\n",
        "from google.cloud import storage\n",
        "\n",
        "import vertexai\n",
        "from vertexai.tuning import sft\n",
        "\n",
        "\n",
        "# Creates single json tuning input data\n",
        "def convert_line_to_jsonl(source_language, target_language, source_sentence, target_sentence):\n",
        "  return json.dumps({\n",
        "      \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": source_language + \": \" + source_sentence + \" \" + target_language + \": \"}]},\n",
        "       {\"role\": \"model\", \"parts\": [{\"text\": target_sentence}]}]}, ensure_ascii=False)\n",
        "\n",
        "\n",
        "# Format conversion function for single file input. Output file will have the same name but in jsonl format.\n",
        "def convert_file_format(input_file, sample_size, source_language_code, target_language_code):\n",
        "  if source_language_code not in language_map or target_language_code not in language_map:\n",
        "    raise ValueError(\"Invalid language code\")\n",
        "  name, ext = os.path.splitext(input_file)\n",
        "  output_file = name + '.jsonl'\n",
        "  if input_file.endswith('.tsv'):\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
        "      open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        reader = csv.reader(infile, delimiter='\\t')\n",
        "        for i, row in enumerate(reader):\n",
        "          if i == sample_size:\n",
        "            break\n",
        "          message = convert_line_to_jsonl(language_map[source_language_code], language_map[target_language_code], row[0], row[1])\n",
        "          outfile.write(message)\n",
        "          outfile.write('\\n')\n",
        "\n",
        "  elif input_file.endswith('.csv'):\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
        "      open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        for i, row in enumerate(reader):\n",
        "          if i == sample_size:\n",
        "            break\n",
        "          message = convert_line_to_jsonl(language_map[source_language_code], language_map[target_language_code], row[0], row[1])\n",
        "          outfile.write(message)\n",
        "          outfile.write('\\n')\n",
        "\n",
        "  elif input_file.endswith('.tmx'):\n",
        "    with open(input_file, 'rb') as infile, \\\n",
        "      open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        tmx_file = tmxfile(infile, 'source_language_code', 'target_language_code')\n",
        "        for i, node in enumerate(tmx_file.unit_iter()):\n",
        "          if i == sample_size:\n",
        "            break\n",
        "          message = convert_line_to_jsonl(language_map[source_language_code], language_map[target_language_code], node.source, node.target)\n",
        "          outfile.write(message)\n",
        "          outfile.write('\\n')\n",
        "\n",
        "  else:\n",
        "    raise ValueError(\"Invalid file type\")\n",
        "\n",
        "  return output_file\n",
        "\n",
        "\n",
        "# Initiates model training\n",
        "def train_model(train_dataset_path, validation_dataset_path, tuned):\n",
        "  vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "  sft_tuning_job = sft.train(\n",
        "    source_model=\"translation-llm-002\",\n",
        "    train_dataset=train_dataset_path,\n",
        "    validation_dataset=validation_dataset_path,\n",
        "    tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\n",
        "  )\n",
        "\n",
        "  # Polling for job completion\n",
        "  while not sft_tuning_job.has_ended:\n",
        "    time.sleep(60)\n",
        "    sft_tuning_job.refresh()\n",
        "\n",
        "  endpoint_short_name = sft_tuning_job.tuned_model_endpoint_name.rsplit('/', 1)[-1]\n",
        "  custom_model_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/models/translation-llm-custom/{endpoint_short_name}\"\n",
        "\n",
        "  print(\"Model: \", custom_model_name)\n",
        "  return custom_model_name\n"
      ],
      "metadata": {
        "id": "dTMTU9SYd3dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset Format Conversion"
      ],
      "metadata": {
        "id": "y8RpI7kE1nF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step convers data to `.jsonl` format for tuning."
      ],
      "metadata": {
        "id": "t-fj4DU6Bsv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_jsonl = convert_file_format(TRAIN_FILE_PATH, TRAIN_DATASET_SAMPLE_SIZE, SOURCE_LANGUAGE_CODE, TARGET_LANGUAGE_CODE)\n",
        "validation_jsonl = convert_file_format(VALIDATION_FILE_PATH, VALIDATION_DATASET_SAMPLE_SIZE, SOURCE_LANGUAGE_CODE, TARGET_LANGUAGE_CODE)\n",
        "GCS_UPLOAD_PATH = GCS_UPLOAD_PATH.rstrip('/')"
      ],
      "metadata": {
        "id": "uNxQzTs1tnw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp {train_jsonl} {GCS_UPLOAD_PATH}\n",
        "!gsutil cp {validation_jsonl} {GCS_UPLOAD_PATH}"
      ],
      "metadata": {
        "id": "_2WaY3a5trun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiate Vertex Tuning Request"
      ],
      "metadata": {
        "id": "-guLLW3wV8Ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tuning is done, the translation model name will be returned to be used for translation requests."
      ],
      "metadata": {
        "id": "wZVCR7wiB6Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = GCS_UPLOAD_PATH + '/' + os.path.basename(train_jsonl)\n",
        "validation_dataset_path = GCS_UPLOAD_PATH + '/' + os.path.basename(validation_jsonl)\n",
        "custom_model_name = train_model(train_dataset_path, validation_dataset_path, TUNED_MODEL_DISPLAY_NAME)"
      ],
      "metadata": {
        "id": "gDa3pbhEqWML"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}